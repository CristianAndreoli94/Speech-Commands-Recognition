# Speech Commands Recognition

<!-- Short repo description (â‰¤â€¯350â€¯characters) -->
Speechâ€‘Commands word spotting on 105â€¯k clips /â€¯35 classes with 1â€¯600â€‘dim spectrograms. Compares meanâ€‘var, MaxAbs &â€¯L2 normalisation and several MLP topologies; best model (1600â€‘140â€‘70â€‘35,â€¯LRâ€¯0.001,â€¯batchâ€¯100) tops 90â€¯% test accuracy with detailed confusionâ€‘matrix analysis.

---

## ðŸŽ¯ Goal
Build a lightweight pipeline that recognises a single spoken word (out of 35) in Googleâ€™s Speech Commands dataset, relying on compact MLPs instead of heavier CNN architectures.

## ðŸ“¦ Dataset
* **Google Speech Commands v0.02** â€“ 105â€¯829 oneâ€‘second WAV clips labelled with one of 35 trigger words, split into train/val/test folds.  
* Each clip is preâ€‘converted to a **1â€¯600â€‘element logâ€‘spectrogram vector** (40â€¯Ã—â€¯40) for quick experimentation.

## ðŸ› ï¸ Pipeline
1. **Exploration & visualisation** â€“ sanityâ€‘check spectrograms and compute mean/variance images.  
2. **Normalisation study** â€“ compare *meanâ€‘var*, *MaxAbs* and *L2* scaling; L2 + small batches converge quickest.  
3. **Model sweep** â€“ MLPs with 0â€“2 hidden layers; gridâ€‘search over batch size, learning rate and epochs.  
4. **Chosen model** â€“ `1600 â†’ 140 â†’ 70 â†’ 35`, LRâ€¯1eâ€‘3, batchâ€¯100: stable >â€¯90â€¯% validation and test accuracy, <â€¯150â€¯s per epoch on CPU.  
5. **Evaluation** â€“ full confusion matrix and perâ€‘class error analysis (classesâ€¯8,â€¯14 andâ€¯28 are hardest).

## ðŸ”¬ Key findings

| Aspect        | Insight                                                                                   |
| ------------- | ----------------------------------------------------------------------------------------- |
| Normalisation | Meanâ€‘var smooths training; L2 edges slightly ahead on final accuracy.                     |
| Batch size    | 100 balances speed and generalisation; very large batches hurt performance.              |
| Depth         | One hidden layer captures most variance; deeper nets risk overâ€‘fitting without gains.    |
